import os
import pickle
from typing import Dict, List, Any
from sklearn.model_selection import StratifiedGroupKFold, train_test_split

import numpy as np
import pandas as pd
import torch


class dataProcessor:
    """
    ë°ì´í„° ë¡œë”©, ì „ì²˜ë¦¬, ë¶„í•  ë° ë¶„ì„ì„ ì´ê´„í•˜ëŠ” í´ë˜ìŠ¤.
    """
    def __init__(self, cfg):
        self.cfg = cfg
        self.data_map: Dict[str, Dict[str, Any]] = {}
        self.train_keys: List[str] = []
        self.val_keys: List[str] = []
        self.test_keys: List[str] = []
        self.dataset_kwargs: Dict[str, Any] = {}
        self.imu_cols, self.ppg_cols, self.sc_cols, self.veh_cols, self.label_cols = [], [], [], [], []
        self.survey_df = None

    def prepare(self) -> None:
        """ë°ì´í„° ì¤€ë¹„ë¥¼ ìœ„í•œ ë©”ì¸ ë©”ì„œë“œ."""        
        self._load_and_restructure_pkl()
        self._load_and_integrate_survey()
        self._split_keys()
        self._prepare_dataset_metadata()
        
        print("ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

    def _load_and_restructure_pkl(self) -> None:
        """Raw PKL íŒŒì¼ì„ ë¡œë“œí•˜ê³ , flat DataFrameì„ ëª¨ë‹¬ë¦¬í‹°ë³„ ë”•ì…”ë„ˆë¦¬ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤."""
        with open(self.cfg.Project.pkl_all, 'rb') as f:
            raw_data = pickle.load(f)

        self.data_map = raw_data.get('all', raw_data)

        for pid, df in self.data_map.items():
            if isinstance(df, pd.DataFrame):
                self.data_map[pid] = {
                    'imu':   df.filter(regex='^imu_'),
                    'ppg':   df.filter(regex='^ppg_'),
                    'sc':    df.filter(regex='^sc_'),
                    'veh':   df.filter(regex='^veh_'),
                    'label': df.filter(regex='^label_')
                }
        print(f"PKL ë¡œë”© ë° ì¬êµ¬ì„± ì™„ë£Œ: {len(self.data_map)}ê°œ í‚¤")

    def _load_and_integrate_survey(self) -> None:
        """Survey CSVë¥¼ ë¡œë“œí•˜ê³ , data_mapì— í†µí•©í•©ë‹ˆë‹¤."""
        try:
            survey_df = pd.read_csv(self.cfg.Project.survey_csv)

            # read_csv ì§í›„, ì²« ë²ˆì§¸ ì»¬ëŸ¼(PID)ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
            pid_col_name = survey_df.columns[0]
            survey_df.set_index(pid_col_name, inplace=True)
            self.survey_df = survey_df # ì¸ë±ìŠ¤ê°€ ì„¤ì •ëœ dfë¥¼ ì €ì¥
            
            # survey_map ìƒì„± ë¡œì§ì€ ì¸ë±ìŠ¤ê°€ ì„¤ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ì•½ê°„ ìˆ˜ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜,
            # í˜„ì¬ ì½”ë“œ(iterrows)ëŠ” ì¸ë±ìŠ¤ ì„¤ì • ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë™ì¼í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
            survey_items = self.survey_df.columns.tolist()
            survey_map = {
                str(int(pid)): row[survey_items].astype(np.float32).to_numpy()
                for pid, row in self.survey_df.iterrows()
            }

            for pid in self.data_map:
                subj_id = pid.split('_')[0]
                self.data_map[pid]['survey'] = survey_map.get(
                    subj_id, np.zeros(len(survey_items), dtype=np.float32)
                )
            print("ğŸ“Š Survey ë°ì´í„° í†µí•© ì™„ë£Œ.")
        except Exception as e:
            print(f"Survey ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}. Survey ë°ì´í„°ë¥¼ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.")
            dummy_survey_len = self.cfg.Encoders.survey['input_dim']
            for pid in self.data_map:
                self.data_map[pid]['survey'] = np.zeros(dummy_survey_len, dtype=np.float32)

    def _split_keys(self) -> None:
        """
        [ìµœì¢… ìˆ˜ì •] StratifiedGroupKFoldë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
        - Step 1: ê° ë°ì´í„° í‚¤ì˜ ëŒ€í‘œ ë¼ë²¨ì„ 'ìµœë¹ˆê°’(mode)'ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        - Step 2: ë¼ë²¨ì´ -100ì¸ í‚¤ëŠ” ê³„ì¸µí™”ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
        - Step 3: ìœ íš¨í•œ ë¼ë²¨ì„ ê°€ì§„ í‚¤ì— ëŒ€í•´ì„œë§Œ ê³„ì¸µì  ê·¸ë£¹ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        - Step 4: -100 ë¼ë²¨ í‚¤ëŠ” í•™ìŠµ(Train) ë°ì´í„°ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
        """
        print("StratifiedGroupKFold ë¶„í•  ì‹œì‘ (Valence+Arousal ìµœë¹ˆê°’ ê¸°ì¤€, -100 ë¼ë²¨ ì œì™¸)...")
        
        all_keys = sorted(self.data_map.keys())
        
        # 1. [ìˆ˜ì •] ìœ íš¨í•œ ë¼ë²¨ì„ ê°€ì§„ í‚¤ì™€ íŒ¨ë”©(-100) ë¼ë²¨ì„ ê°€ì§„ í‚¤ë¥¼ ë¶„ë¦¬
        valid_keys, valid_labels, valid_groups = [], [], []
        padding_keys = []

        for key in all_keys:
            subj_id = key.split('_')[0]
            
            # [í•µì‹¬ ìˆ˜ì •] ëŒ€í‘œ ë¼ë²¨ì„ ìµœë¹ˆê°’(mode)ìœ¼ë¡œ ì¶”ì¶œ
            labels_v = self.data_map[key]['label']['label_valence']
            labels_a = self.data_map[key]['label']['label_arousal']
            
            # -100ì„ ì œì™¸í•œ ìœ íš¨ ë¼ë²¨ë§Œ í•„í„°ë§
            valid_labels_v = labels_v[(labels_v >= 1) & (labels_v < 10)]
            valid_labels_a = labels_a[(labels_a >= 1) & (labels_a < 10)]
            # ìœ íš¨ ë¼ë²¨ì´ í•˜ë‚˜ë¼ë„ ì¡´ì¬í•˜ë©´ ìµœë¹ˆê°’ì„ ê³„ì‚°
            if not valid_labels_v.empty and not valid_labels_a.empty:
                raw_v = valid_labels_v.mode().iloc[0]
                raw_a = valid_labels_a.mode().iloc[0]

                label_v = 0 if raw_v < 4 else (1 if raw_v < 7 else 2)
                label_a = 0 if raw_a < 4 else (1 if raw_a < 7 else 2)
                combined_label = label_v * 3 + label_a
                
                valid_keys.append(key)
                valid_labels.append(combined_label)
                valid_groups.append(subj_id)
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ ì „ì²´ê°€ -100 ë¼ë²¨ì¸ ê²½ìš°
                padding_keys.append(key)
        
        print(f"ì „ì²´ í‚¤: {len(all_keys)}ê°œ | ê³„ì¸µí™” ëŒ€ìƒ(ìœ íš¨ ë¼ë²¨): {len(valid_keys)}ê°œ | íŒ¨ë”© ë¼ë²¨: {len(padding_keys)}ê°œ")

        # 2. Test Set ë¶„ë¦¬ (Config ê¸°ì¤€, ì „ì²´ í‚¤ì—ì„œ ìˆ˜í–‰)
        test_subj_set = set(self.cfg.Data.test_subjects)
        self.test_keys = [key for key in all_keys if key.split('_')[0] in test_subj_set]
        
        # 3. ìœ íš¨í•œ í‚¤ë“¤ ì¤‘ì—ì„œ Test Setì— ì†í•˜ì§€ ì•ŠëŠ” í‚¤ë“¤ë¡œ Train/Val ë¶„í•  ìˆ˜í–‰
        non_test_valid_indices = [i for i, g in enumerate(valid_groups) if g not in test_subj_set]
        train_val_keys = [valid_keys[i] for i in non_test_valid_indices]
        train_val_labels = [valid_labels[i] for i in non_test_valid_indices]
        train_val_groups = [valid_groups[i] for i in non_test_valid_indices]

        # [ì•ˆì „ì¥ì¹˜] ë¶„í• í•  ê·¸ë£¹ì´ ìˆëŠ”ì§€ í™•ì¸
        if not train_val_groups:
            raise ValueError("í…ŒìŠ¤íŠ¸ì…‹ì„ ì œì™¸í•œ í›„ í•™ìŠµ/ê²€ì¦ì— ì‚¬ìš©í•  ìœ íš¨í•œ í”¼ì‹¤í—˜ì ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤. configì˜ test_subjectsë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        num_total_subj = len(set(train_val_groups))
        num_val_subj = len(self.cfg.Data.val_subjects)
        n_splits = round(num_total_subj / num_val_subj) if num_val_subj > 0 and num_total_subj > num_val_subj else 5

        sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=self.cfg.Project.seed)
        
        try:
            train_indices, val_indices = next(sgkf.split(train_val_keys, train_val_labels, train_val_groups))
            self.train_keys = [train_val_keys[i] for i in train_indices]
            self.val_keys = [train_val_keys[i] for i in val_indices]
        except ValueError:
            print("StratifiedGroupKFold ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ. ê·¸ë£¹ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ì¼ë°˜ ë¶„í• ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # fallback to simple random split based on subjects
            unique_train_val_groups = sorted(list(set(train_val_groups)))
            train_pids, val_pids = train_test_split(unique_train_val_groups, test_size=1/n_splits, random_state=self.cfg.Project.seed)
            
            self.train_keys = [k for k, g in zip(train_val_keys, train_val_groups) if g in train_pids]
            self.val_keys = [k for k, g in zip(train_val_keys, train_val_groups) if g in val_pids]

        # 4. íŒ¨ë”© í‚¤ ì²˜ë¦¬: Test Setì— ì†í•˜ì§€ ì•ŠëŠ” íŒ¨ë”© í‚¤ë“¤ì€ ëª¨ë‘ Train Setì— ì¶”ê°€
        padding_keys_for_train = [key for key in padding_keys if key.split('_')[0] not in test_subj_set]
        self.train_keys.extend(padding_keys_for_train)
        
        # 5. ìµœì¢… ì •ë¦¬ ë° í™•ì¸
        self.train_keys = sorted(list(set(self.train_keys)))
        val_pids_from_split = sorted(list(set([k.split('_')[0] for k in self.val_keys])))
        self.cfg.Data.val_subjects = val_pids_from_split
        
        print(f"Configì˜ val_subjectsê°€ ë¶„í•  ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸ë¨: {val_pids_from_split}")
        if not all([self.train_keys, self.val_keys, self.test_keys]):
            raise RuntimeError("Train/Val/Test ì„¸íŠ¸ ì¤‘ í•˜ë‚˜ ì´ìƒì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
        print(f"ë¶„í•  ì™„ë£Œ: Train {len(self.train_keys)}, Val {len(self.val_keys)}, Test {len(self.test_keys)}ê°œ í‚¤")

    def _prepare_dataset_metadata(self) -> None:
        """ë°ì´í„°ì…‹ ìƒì„±ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„°(ì»¬ëŸ¼ ëª©ë¡ ë“±)ë¥¼ ì¤€ë¹„í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤."""
        first_key = self.train_keys[0]
        sample_data = self.data_map[first_key]

        self.imu_cols = sample_data['imu'].columns.tolist()
        self.ppg_cols = sample_data['ppg'].columns.tolist()
        self.sc_cols = sample_data['sc'].columns.tolist()
        self.veh_cols = sample_data['veh'].columns.tolist()
        self.label_cols = sample_data['label'].columns.tolist()
        survey_len = len(sample_data['survey'])

        assert len(self.imu_cols) == self.cfg.Encoders.imu['input_dim'], "IMU input_dim ë¶ˆì¼ì¹˜"
        assert len(self.veh_cols) == self.cfg.Encoders.veh['input_dim'], "Vehicle input_dim ë¶ˆì¼ì¹˜"
        assert survey_len == self.cfg.Encoders.survey['input_dim'], "Survey input_dim ë¶ˆì¼ì¹˜"

        self.dataset_kwargs = {
            "data_map": self.data_map,
            "survey_df": self.survey_df,
            "imu_cols": self.imu_cols,
            "ppg_cols": self.ppg_cols,
            "sc_cols": self.sc_cols,
            "veh_cols": self.veh_cols,
            "label_cols": self.label_cols,
            "fs": self.cfg.Data.fs,
        }

class TrainerBase(dataProcessor):
    """
    [ê°œì„ ] DataProcessorë¥¼ ìƒì†ë°›ì•„ ë°ì´í„° ì¤€ë¹„ ê¸°ëŠ¥ì„ ë‚´ì¬í™”.
    ê³µí†µì ì¸ í•™ìŠµ, ê²€ì¦, ì²´í¬í¬ì¸íŠ¸ ìœ í‹¸ë¦¬í‹°ë¥¼ ì œê³µí•˜ëŠ” ê¸°ë³¸ íŠ¸ë ˆì´ë„ˆ.
    """
    def __init__(self, cfg, model, optimizer, loss_fn, device):
        super().__init__(cfg) # DataProcessor ì´ˆê¸°í™”
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.device = device
        self.current_epoch = 0

    def _load_state(self, path: str) -> None:
        """ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ê³¼ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print(f"ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {path}")
        state = torch.load(path, map_location=self.device)
        
        # [ê°œì„ ] state ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ê°€ ì‹¤ì œ ëª¨ë¸ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ë©° ì•ˆì „í•˜ê²Œ ë¡œë“œ
        for name, module in self.nets.items():
            if name in state.get('nets', {}):
                module.load_state_dict(state['nets'][name])
        
        for name, module in self.projs.items():
            if name in state.get('projs', {}):
                module.load_state_dict(state['projs'][name])

        # [ê°œì„ ] ê° headì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ê³  ë¡œë“œ
        if hasattr(self, 'valence_head') and 'valence_head' in state:
            self.valence_head.load_state_dict(state['valence_head'])
        if hasattr(self, 'arousal_head') and 'arousal_head' in state:
            self.arousal_head.load_state_dict(state['arousal_head'])
        if hasattr(self, 'motion_head') and 'motion_head' in state:
            self.motion_head.load_state_dict(state['motion_head'])

        if 'optimizer' in state:
            self.optimizer.load_state_dict(state['optimizer'])
        if 'scaler' in state and hasattr(self, 'scaler'):
            self.scaler.load_state_dict(state['scaler'])
        
        self.current_epoch = state.get('epoch', 0)
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ. Epoch {self.current_epoch}ì—ì„œ ì¬ì‹œì‘í•©ë‹ˆë‹¤.")