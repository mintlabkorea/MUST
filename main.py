import os
import csv
import numpy as np
import torch
import warnings
warnings.simplefilter("ignore", FutureWarning)
warnings.filterwarnings("ignore", message=".*use_reentrant.*")
warnings.filterwarnings("ignore", message=".*None of the inputs have requires_grad=True.*")

import numpy as np
import random
from tqdm import tqdm
from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

# --- ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
import matplotlib.pyplot as plt
import seaborn as sns

# --- ëª¨ë“ˆ ì„í¬íŠ¸ ---
from config.config import Config
from trainers.base_trainer import dataProcessor

from trainers.motion_trainer import MotionTrainer
from trainers.emotion_trainer import EmotionTrainer
from trainers.fusion_trainer_v28 import FusionTrainer as ContextExpertTrainer
from trainers.totact_trainer import BaselineTrainer, EnhancerTrainer
from models.totact_models import TOT_BaselineAblation, ACT_BaselineAblation, EnhancedTOTModel, EnhancedACTModel
from data.code.pkl_dataloader_totact import PKLMultiModalDatasetBaseline
from data.loader import make_multitask_loader


# ============================================================
# ìœ í‹¸
# ============================================================
def set_seed(seed: int):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)


def ensure_dirs():
    os.makedirs("weights", exist_ok=True)
    os.makedirs("results", exist_ok=True)

@torch.no_grad()
def audit_tot_split(model, loader, device, pooling: str = "mean", desc: str = "[AUDIT] TOT"):
    """
    TOT í‰ê°€ì— ì‹¤ì œë¡œ ëª‡ ê°œ ìƒ˜í”Œì´ ë“¤ì–´ê°€ëŠ”ì§€, ì–´ë””ì„œ ë²„ë ¤ì§€ëŠ”ì§€, í´ë˜ìŠ¤ ë¶„í¬ê°€ ì–´ë–¤ì§€ ìš”ì•½.
    - ë¼ë²¨ ìš°ì„ ìˆœìœ„: 'label' -> 'label_act'
    - ì‹œí€€ìŠ¤â†’ì›ì¼ ë•Œ pooling('mean'|'last'|'max') ì ìš©
    - -100 ë§ˆìŠ¤í‚¹ ì ìš©
    ë°˜í™˜: (stats_dict, true_hist(np), pred_hist(np))
    """
    def pick_logits(out):
        if isinstance(out, dict):
            for k in ("tot_logits","logits","out"):
                if k in out: return out[k]
            raise ValueError("No tot logits in dict")
        return out

    def pool_seq_logits(x, mode):
        if x.dim()!=3: return x
        if mode=="mean": return x.mean(dim=1)
        if mode=="last": return x[:, -1, :]
        if mode=="max":  return x.max(dim=1).values
        raise ValueError(mode)

    model.eval()
    stats = dict(
        batches=0,         # ì²˜ë¦¬í•œ ë°°ì¹˜ ìˆ˜
        elems=0,           # ë¼ë²¨ ì›ì†Œ ìˆ˜(ë§ˆìŠ¤í¬ ì „)
        valid=0,           # ë§ˆìŠ¤í¬ í†µê³¼í•œ ìœ íš¨ í‘œë³¸ ìˆ˜
        bc_batches=0,      # (B,C) ëª¨ì–‘ ë°°ì¹˜ ìˆ˜
        btc_batches=0,     # (B,T,C) ëª¨ì–‘ ë°°ì¹˜ ìˆ˜
        seq2one_batches=0, # (B,T,C)+(B,) / (B,1) â†’ í’€ë§ ì ìš©ëœ ë°°ì¹˜ ìˆ˜
        seq2seq_batches=0, # (B,T,C)+(B,T) ê·¸ëŒ€ë¡œ í‰ê°€ëœ ë°°ì¹˜ ìˆ˜
        dropped_shape=0,   # ëª¨ì–‘ ë¯¸ìŠ¤ë§¤ì¹˜ë¡œ ìŠ¤í‚µëœ ë°°ì¹˜ ìˆ˜
    )
    true_hist = None
    pred_hist = None
    C_seen = None

    for batch in tqdm(loader, desc=desc):
        for k,v in batch.items():
            if torch.is_tensor(v):
                batch[k] = v.to(device)

        out   = model(batch)
        logits = pick_logits(out)  # (B,C) or (B,T,C)

        tgt = batch.get("label", batch.get("label_act", None))
        if tgt is None:
            print("[WARN] batch without label/label_act â€” skipped")
            stats["dropped_shape"] += 1
            continue

        # case A: (B,T,C) + (B,) or (B,1)  => seq2one
        if logits.dim()==3 and (tgt.dim()==1 or (tgt.dim()==2 and tgt.size(1)==1)):
            logits_eval = pool_seq_logits(logits, pooling) # (B,C)
            targets_eval = tgt.long() if tgt.dim()==1 else tgt.squeeze(1).long()
            pred = logits_eval.argmax(-1)
            true = targets_eval
            mask = (true != -100)
            n_valid = int(mask.sum().item())
            stats["seq2one_batches"] += 1
            stats["bc_batches"] += 1
            C_cur = logits_eval.size(-1)

        # case B: (B,T,C) + (B,T)          => seq2seq
        elif logits.dim()==3 and tgt.dim()==2 and logits.size(1)==tgt.size(1):
            pred = logits.argmax(-1).reshape(-1)   # (B*T,)
            true = tgt.reshape(-1)
            mask = (true != -100)
            n_valid = int(mask.sum().item())
            stats["seq2seq_batches"] += 1
            stats["btc_batches"] += 1
            C_cur = logits.size(-1)

        # case C: (B,C) + (B,)
        elif logits.dim()==2 and tgt.dim()==1:
            pred = logits.argmax(-1)               # (B,)
            true = tgt
            mask = (true != -100)
            n_valid = int(mask.sum().item())
            stats["bc_batches"] += 1
            C_cur = logits.size(-1)

        else:
            print(f"[WARN] shape mismatch skipped: logits={tuple(logits.shape)} tgt={tuple(tgt.shape)}")
            stats["dropped_shape"] += 1
            continue

        # ëˆ„ì 
        stats["batches"] += 1
        stats["elems"]   += int(true.numel())
        stats["valid"]   += n_valid

        C_seen = C_cur if C_seen is None else max(C_seen, C_cur)
        if n_valid > 0:
            if true_hist is None:
                true_hist = torch.zeros(C_cur, dtype=torch.long)
                pred_hist = torch.zeros(C_cur, dtype=torch.long)
            # ì‚¬ì´ì¦ˆ ë³€ë™ ë°©ì§€
            if true_hist.numel() < C_cur:
                true_hist = torch.nn.functional.pad(true_hist, (0, C_cur-true_hist.numel()))
                pred_hist = torch.nn.functional.pad(pred_hist, (0, C_cur-pred_hist.numel()))
            true_hist += torch.bincount(true[mask].detach().cpu(), minlength=C_cur)
            pred_hist += torch.bincount(pred[mask].detach().cpu(), minlength=C_cur)

    # ìš”ì•½ ì¶œë ¥
    print("---- TOT AUDIT SUMMARY ----")
    for k,v in stats.items():
        print(f"{k:>18}: {v}")
    if true_hist is not None:
        print(" label histogram:", true_hist.tolist())
        print(" pred  histogram:", pred_hist.tolist())
    else:
        print(" (no valid samples)")

    return stats, (None if true_hist is None else true_hist.numpy()), (None if pred_hist is None else pred_hist.numpy())

# ============================================================
# Collate í•¨ìˆ˜ (TOT/ACT ê³µìš©)
# ============================================================
def collate_fn_baseline(batch):
    keys = set().union(*[b.keys() for b in batch])
    out = {}
    for k in keys:
        vals = [b[k] for b in batch if k in b]
        if not vals:
            continue
        if isinstance(vals[0], torch.Tensor):
            continue
        out[k] = vals

    for k in keys:
        vals = [b[k] for b in batch if k in b and isinstance(b[k], torch.Tensor)]
        if not vals:
            continue
        if all(v.ndim == 0 for v in vals):
            out[k] = torch.stack(vals).long() if k == "label" else torch.stack(vals)
            continue

        padding_value = -100.0 if k == "label" else 0.0
        if all(v.ndim == 1 for v in vals):
            out[k] = pad_sequence(vals, batch_first=True, padding_value=padding_value)
        elif all(v.ndim == 2 for v in vals):
            out[k] = pad_sequence(vals, batch_first=True, padding_value=padding_value)
        else:
            fixed, max_T = [], max(v.shape[0] for v in vals)
            for v in vals:
                v = v[:, None] if v.ndim == 1 else v
                T, D = v.shape[:2]
                if T < max_T:
                    pad = torch.full((max_T - T, D), padding_value, dtype=v.dtype, device=v.device)
                    v = torch.cat([v, pad], dim=0)
                fixed.append(v)
            out[k] = torch.stack(fixed, dim=0)
    return out


# ============================================================
# ë¡œë” ìƒì„±
# ============================================================
def make_totact_loaders(cfg, dp, task_name: str, batch_size: int = None):
    ds_kwargs = {
        "data_map": dp.data_map,
        "veh_cols": dp.veh_cols,
        "fs": cfg.Data.fs,
        "mode": task_name,
        "window_sec": cfg.Data.window_sec_mot,
        "window_stride": cfg.Data.window_stride_mot,
    }
    train_ds = PKLMultiModalDatasetBaseline(participant_ids=dp.train_keys, **ds_kwargs)
    val_ds   = PKLMultiModalDatasetBaseline(participant_ids=dp.val_keys, **ds_kwargs)
    test_ds  = PKLMultiModalDatasetBaseline(participant_ids=dp.test_keys, **ds_kwargs)

    bs = batch_size or cfg.Data.batch_size
    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,  collate_fn=collate_fn_baseline)
    val_loader   = DataLoader(val_ds,   batch_size=bs, shuffle=False, collate_fn=collate_fn_baseline)
    test_loader  = DataLoader(test_ds,  batch_size=bs, shuffle=False, collate_fn=collate_fn_baseline)
    return train_loader, val_loader, test_loader


# ============================================================
# í‰ê°€/ì‹œê°í™”
# ============================================================
def save_confusion(y_true, y_pred, save_path: str, title: str = "Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.title(title); plt.xlabel("Predicted"); plt.ylabel("True")
    plt.tight_layout(); plt.savefig(save_path); plt.close()


def save_act_line_plot(true_values, pred_values, save_path: str, title: str = "ACT Prediction vs True"):
    plt.figure(figsize=(10, 4))
    plt.plot(true_values, label="True")
    plt.plot(pred_values, label="Pred")
    plt.legend(); plt.title(title); plt.xlabel("Time"); plt.ylabel("ACT (S)")
    plt.tight_layout(); plt.savefig(save_path); plt.close()

@torch.no_grad()
def evaluate_enhancer_tot(model, loader, device, pooling: str = "mean"):
    """
    Final step ì „ìš© TOT í‰ê°€ (Enhancer)
    - ì¶œë ¥: í…ì„œ ë˜ëŠ” dict({'tot_logits'|'logits'|'out'})
    - ë¼ë²¨: 'label' ìš°ì„  (ì—†ìœ¼ë©´ 'label_act')
    - ì‹œí€€ìŠ¤â†’ì› ê³¼ì œ ì‹œ ì‹œê°„ì¶• í’€ë§(mode: mean/last/max)
    - ignore_index = -100 ë§ˆìŠ¤í‚¹
    ë°˜í™˜: acc(float), (y_true, y_pred)  # í•„ìš”í•˜ë©´ ì‹œê°í™”ì— ì‚¬ìš©
    """
    def _pick_logits(out):
        if isinstance(out, dict):
            for k in ("tot_logits", "logits", "out"):
                if k in out:
                    return out[k]
            raise ValueError("dict output has no 'tot_logits'/'logits'/'out'")
        elif torch.is_tensor(out):
            return out
        else:
            raise TypeError(f"Unexpected output type: {type(out)}")

    def _pool_seq_logits(logits: torch.Tensor, mode: str):
        if logits.dim() != 3:
            return logits
        if mode == "mean":
            return logits.mean(dim=1)
        elif mode == "last":
            return logits[:, -1, :]
        elif mode == "max":
            return logits.max(dim=1).values
        else:
            raise ValueError(f"Unknown pooling mode: {mode}")

    model.eval()
    all_pred, all_true = [], []

    for batch in tqdm(loader, desc="[TEST] Enhanced-TOT"):
        # move to device
        for k, v in batch.items():
            if torch.is_tensor(v):
                batch[k] = v.to(device)

        out = model(batch)
        logits = _pick_logits(out)  # (B,C) or (B,T,C)
        tgt = batch.get("label", batch.get("label_act", None))
        if tgt is None:
            continue

        # ì‹œí€€ìŠ¤â†’ì›: logits (B,T,C) & tgt (B,) or (B,1)
        if logits.dim() == 3 and (tgt.dim() == 1 or (tgt.dim() == 2 and tgt.size(1) == 1)):
            logits_eval = _pool_seq_logits(logits, pooling)    # (B,C)
            targets_eval = tgt.long() if tgt.dim() == 1 else tgt.squeeze(1).long()
            pred = logits_eval.argmax(-1)                      # (B,)
            true = targets_eval                                # (B,)
            mask = (true != -100)
            if mask.any():
                all_pred.append(pred[mask].detach().cpu())
                all_true.append(true[mask].detach().cpu())
            continue

        # ì‹œí€€ìŠ¤â†’ì‹œí€€ìŠ¤: (B,T,C) vs (B,T)
        if logits.dim() == 3 and tgt.dim() == 2 and logits.size(1) == tgt.size(1):
            pred = logits.argmax(-1).reshape(-1)               # (B*T,)
            true = tgt.reshape(-1)                             # (B*T,)
            mask = (true != -100)
            if mask.any():
                all_pred.append(pred[mask].detach().cpu())
                all_true.append(true[mask].detach().cpu())
            continue

        # ë°°ì¹˜ ë‹¨ì¼: (B,C) vs (B,)
        if logits.dim() == 2 and tgt.dim() == 1:
            pred = logits.argmax(-1)                           # (B,)
            true = tgt                                         # (B,)
            mask = (true != -100)
            if mask.any():
                all_pred.append(pred[mask].detach().cpu())
                all_true.append(true[mask].detach().cpu())
            continue

        print(f"[WARN] evaluate_enhancer_tot shape mismatch: logits={tuple(logits.shape)}, tgt={tuple(tgt.shape)}")

    if len(all_true) == 0:
        return 0.0, (None, None)

    import numpy as np
    from sklearn.metrics import accuracy_score
    y_pred = torch.cat(all_pred).numpy()
    y_true = torch.cat(all_true).numpy()
    acc = accuracy_score(y_true, y_pred)
    return float(acc), (y_true, y_pred)



@torch.no_grad()
def evaluate_enhancer_act(model, loader, device):
    """
    Enhanced ACT evaluator
    - Supports both legacy (B,T,3) and current (B,T,1) outputs
    - Aligns time length with labels and masks -100.0
    """
    model.eval()
    all_pred_min, all_true = [], []

    for batch in tqdm(loader, desc="[TEST] Enhanced-ACT"):
        # move tensors to device
        for k, v in batch.items():
            if isinstance(v, torch.Tensor):
                batch[k] = v.to(device)

        # forward (tensor or dict)
        out = model(batch)
        pred = out
        if isinstance(out, dict):
            for k in ("act_preds", "preds", "act_logits", "logits", "out"):
                if k in out:
                    pred = out[k]
                    break

        # unify prediction shape
        # - (B,T,1) -> (B,T)
        # - (B,T,3) legacy -> use channel 1 (min_ACT) if available; else first channel
        if pred.dim() == 3:
            C = pred.size(-1)
            if C == 1:
                pred_use = pred.squeeze(-1)           # (B,T)
            elif C >= 2:
                pred_use = pred[..., 1]               # min_ACT channel
            else:
                raise ValueError(f"Unexpected pred channels: {C}")
        else:
            pred_use = pred                           # (B,T) or (B,)

        # labels
        true = batch["label"]                         # (B,T) or (B,) or (B,T,1)
        if true.dim() == 3 and true.size(-1) == 1:
            true = true.squeeze(-1)
        if true.dim() == 1 and pred_use.dim() == 2 and pred_use.size(0) == true.size(0):
            true = true.unsqueeze(1)

        # time alignment (crop to min length)
        if pred_use.dim() == 2 and true.dim() == 2 and pred_use.size(1) != true.size(1):
            T = min(pred_use.size(1), true.size(1))
            pred_use = pred_use[:, :T]
            true     = true[:, :T]

        # mask & gather
        mask = (true != -100.0) & torch.isfinite(true) & torch.isfinite(pred_use)
        if not mask.any():
            continue
        all_pred_min.append(pred_use[mask].detach().cpu())
        all_true.append(true[mask].detach().cpu())

    if not all_true:
        return float("inf"), float("inf"), None, None

    y_pred = torch.cat(all_pred_min).numpy()
    y_true = torch.cat(all_true).numpy()
    mse = ((y_pred - y_true) ** 2).mean()
    rmse = float(np.sqrt(mse))
    return mse, rmse, y_true, y_pred

@torch.no_grad()
def _eval_act_raw_series(model, loader, device):
    """
    (true, pred_min) ì‹œê³„ì—´ì„ í•˜ë‚˜ë¡œ ì´ì–´ ë¶™ì—¬ ë°˜í™˜.
    - ëª¨ë¸ ì¶œë ¥: tensor ë˜ëŠ” dict({'act_preds'|'preds'|'logits'|'out'})
    - ë¼ë²¨: batch['label'] ë˜ëŠ” batch['label_act']
    - (B,T,1)/(B,T) ëª¨ë‘ ì²˜ë¦¬
    """
    def _pick_preds(out):
        if isinstance(out, dict):
            for k in ('act_preds','preds','act_logits','logits','out'):
                if k in out: return out[k]
            raise ValueError("No ACT outputs in dict")
        return out

    model.eval()
    all_true, all_pred = [], []
    for batch in tqdm(loader, desc="[EVAL] ACT (raw)"):
        for k,v in batch.items():
            if torch.is_tensor(v): batch[k] = v.to(device)

        out   = model(batch)
        preds = _pick_preds(out)           # (B,T,1) or (B,T)
        true  = batch.get('label_act', batch.get('label'))

        # preds -> (B,T)
        if preds.dim()==3 and preds.size(-1)==1: preds = preds.squeeze(-1)
        elif preds.dim()==3:                      preds = preds[..., 0]  # êµ¬ë²„ì „ ê°€ë“œ

        # true -> (B,T)
        if true.dim()==3 and true.size(-1)==1: true = true.squeeze(-1)
        if true.dim()==1 and preds.dim()==2 and preds.size(0)==true.size(0):
            true = true.unsqueeze(1)

        # ê¸¸ì´ ë³´ì •
        if preds.dim()==2 and true.dim()==2 and preds.size(1)!=true.size(1):
            T = min(preds.size(1), true.size(1))
            preds, true = preds[:, :T], true[:, :T]

        mask = (true != -100.0) & torch.isfinite(true) & torch.isfinite(preds)
        if mask.any():
            all_true.append(true[mask].detach().cpu())
            all_pred.append(preds[mask].detach().cpu())

    if not all_true:
        return None, None

    y_true = torch.cat(all_true).numpy()
    y_pred = torch.cat(all_pred).numpy()
    return y_true, y_pred


def plot_act_for_subject(model, cfg, dp, participant_id, split="test", out_dir="results"):
    """
    íŠ¹ì • ì°¸ê°€ì 1ëª…ë§Œ ëŒ€ìƒìœ¼ë¡œ ACT ë¼ì¸í”Œë¡¯ ì €ì¥.
    split: 'train' | 'val' | 'test'
    íŒŒì¼ëª…: results/act_{split}_subj-{ID}.png
    """
    os.makedirs(out_dir, exist_ok=True)
    # ë¡œë” ìƒì„±
    ds_kwargs = {
        'data_map': dp.data_map,
        'veh_cols': dp.veh_cols,
        'fs': cfg.Data.fs,
        'mode': 'act',
        'window_sec': cfg.Data.window_sec_mot,
        'window_stride': cfg.Data.window_stride_mot,
    }
    from torch.utils.data import DataLoader
    from data.code.pkl_dataloader_totact import PKLMultiModalDatasetBaseline

    if split == "train":
        keys = [k for k in dp.train_keys if k == participant_id]
    elif split == "val":
        keys = [k for k in dp.val_keys if k == participant_id]
    else:
        keys = [k for k in dp.test_keys if k == participant_id]

    if not keys:
        print(f"[WARN] subject {participant_id} not found in {split} split")
        return None

    ds = PKLMultiModalDatasetBaseline(participant_ids=keys, **ds_kwargs)
    loader = DataLoader(ds, batch_size=cfg.Data.batch_size, shuffle=False,
                        collate_fn=collate_fn_baseline)

    y_true, y_pred = _eval_act_raw_series(model, loader, cfg.Project.device)
    if y_true is None:
        print(f"[WARN] no valid ACT labels for subject {participant_id} ({split})")
        return None

    # ì €ì¥
    fig_path = os.path.join(out_dir, f"act_{split}_subj-{participant_id}.png")
    plt.figure(figsize=(10,5))
    plt.plot(y_true, label="True")
    plt.plot(y_pred, label="Predicted")
    plt.title(f"ACT Prediction vs. True (subj={participant_id}, split={split})")
    plt.xlabel("Test Time Index"); plt.ylabel("ACT (s)")
    plt.legend(); plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout(); plt.savefig(fig_path, dpi=150); plt.close()
    print(f"[SAVE] {fig_path}")
    return fig_path


@torch.no_grad()
def plot_act_by_scenario(model, loader, device, out_dir="results", prefix="act_by_scn"):
    """
    ë¡œë”ë¥¼ ê·¸ëŒ€ë¡œ ë°›ì•„, ë°°ì¹˜ì˜ 'scenario' í‚¤ê°€ ìˆìœ¼ë©´ ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ë¶„ë¦¬í•´ì„œ í”Œë¡¯ ì €ì¥.
    - ì—†ìœ¼ë©´ ì „ì²´ë¥¼ 'all'ë¡œ ì €ì¥.
    íŒŒì¼ëª…: results/{prefix}_{scenario}.png
    """
    os.makedirs(out_dir, exist_ok=True)

    def _pick_preds(out):
        if isinstance(out, dict):
            for k in ('act_preds','preds','act_logits','logits','out'):
                if k in out: return out[k]
            raise ValueError("No ACT outputs in dict")
        return out

    buckets = {}  # scenario -> {'true': [tensor...], 'pred':[tensor...]}

    model.eval()
    for batch in tqdm(loader, desc="[EVAL] ACT by scenario"):
        for k,v in batch.items():
            if torch.is_tensor(v): batch[k] = v.to(device)

        out   = model(batch)
        preds = _pick_preds(out)
        true  = batch.get('label_act', batch.get('label'))

        # preds -> (B,T)
        if preds.dim()==3 and preds.size(-1)==1: preds = preds.squeeze(-1)
        elif preds.dim()==3:                      preds = preds[..., 0]
        if true.dim()==3 and true.size(-1)==1:    true  = true.squeeze(-1)
        if true.dim()==1 and preds.dim()==2 and preds.size(0)==true.size(0):
            true = true.unsqueeze(1)

        if preds.dim()==2 and true.dim()==2 and preds.size(1)!=true.size(1):
            T = min(preds.size(1), true.size(1))
            preds, true = preds[:, :T], true[:, :T]

        mask = (true != -100.0) & torch.isfinite(true) & torch.isfinite(preds)
        if not mask.any(): 
            continue

        # ì‹œë‚˜ë¦¬ì˜¤ ì¶”ì¶œ (ì—†ìœ¼ë©´ 'all')
        scn = batch.get('scenario', None)
        if scn is None:
            key = "all"
        else:
            # í…ì„œ / ë¬¸ìì—´ / ìˆ«ì ëª¨ë‘ ëŒ€ì‘
            if torch.is_tensor(scn):
                key = str(scn.detach().cpu().tolist())
            elif isinstance(scn, (list, tuple)):
                key = str(list(scn))  # ë°°ì¹˜ ë‹¨ìœ„ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆìŒ
            else:
                key = str(scn)

        entry = buckets.setdefault(key, {'true': [], 'pred': []})
        entry['true'].append(true[mask].detach().cpu())
        entry['pred'].append(preds[mask].detach().cpu())

    # ì €ì¥
    saved = []
    for scn, d in buckets.items():
        if not d['true']: 
            continue
        y_true = torch.cat(d['true']).numpy()
        y_pred = torch.cat(d['pred']).numpy()

        out_path = os.path.join(out_dir, f"{prefix}_{scn}.png")
        plt.figure(figsize=(10,5))
        plt.plot(y_true, label="True")
        plt.plot(y_pred, label="Predicted")
        plt.title(f"ACT Prediction vs. True (scenario={scn})")
        plt.xlabel("Time Index"); plt.ylabel("ACT (s)")
        plt.legend(); plt.grid(True, linestyle='--', alpha=0.6)
        plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()
        print(f"[SAVE] {out_path}")
        saved.append(out_path)

    if not saved:
        print("[WARN] No ACT plots saved (no valid labels or buckets)")
    return saved

def _smooth(x: np.ndarray, fs: int, smooth_sec: float):
    if smooth_sec is None or smooth_sec <= 0:
        return x
    k = max(1, int(round(fs * smooth_sec)))
    if k == 1: return x
    w = np.ones(k) / k
    return np.convolve(x, w, mode="same")

def _rolling_mae(y_true, y_pred, fs, window_sec=8.0, step_sec=2.0):
    W = max(1, int(round(fs*window_sec)))
    S = max(1, int(round(fs*step_sec)))
    n = len(y_true)
    starts = list(range(0, max(1, n - W + 1), S))
    maes = []
    for s in starts:
        e = np.abs(y_true[s:s+W] - y_pred[s:s+W])
        if len(e) < W: break
        maes.append(e.mean())
    return np.array(starts), np.array(maes), W

def _detect_spikes(y_true, fs, thr=14.0, min_gap_sec=4.0):
    """
    ê°„ë‹¨ ìŠ¤íŒŒì´í¬ íƒì§€: true >= thrì¸ êµ¬ê°„ì˜ ì‹œì‘ì ë“¤ë§Œ ì¶”ë ¤ì„œ ì´ë²¤íŠ¸ë¡œ ë´„.
    ì´ë²¤íŠ¸ ê°„ ìµœì†Œ ê°„ê²© min_gap_sec ë³´ì¥.
    """
    idx = np.where(y_true >= thr)[0]
    if len(idx) == 0: return []
    events = [idx[0]]
    min_gap = int(round(fs*min_gap_sec))
    for i in idx[1:]:
        if i - events[-1] >= min_gap:
            events.append(i)
    return events

def _save_case_plot(y_true, y_pred, fs, s, e, out_path, title):
    x = np.arange(s, e)
    plt.figure(figsize=(10,4))
    plt.plot(x, y_true[s:e], label="True")
    plt.plot(x, y_pred[s:e], label="Predicted")
    plt.title(title)
    plt.xlabel("Test Time Index"); plt.ylabel("ACT (s)")
    plt.grid(True, linestyle="--", alpha=0.6)
    plt.legend()
    plt.tight_layout()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    plt.savefig(out_path, dpi=150)
    plt.close()

@torch.no_grad()
def make_act_case_studies(model,
                          cfg,
                          dp,
                          participant_id: str,
                          split: str = "val",
                          window_sec: float = 8.0,
                          step_sec: float = 2.0,
                          top_k: int = 5,
                          smooth_sec: float = 0.25,
                          spike_thr: float = 14.0,
                          spike_pad_sec: float = 3.0,
                          out_root: str = "results/case_studies"):
    """
    1) ì „êµ¬ê°„ ë¡¤ë§ MAE ê¸°ì¤€ best/worst
    2) ê¸‰ì •ê±° ìŠ¤íŒŒì´í¬(thr ì´ìƒ) ì£¼ë³€ ìœˆë„ìš° ê¸°ì¤€ best/worst
    ë¥¼ ê°ê° top_kì”© PNGë¡œ ì €ì¥ + summary.csv ì €ì¥
    """
    # --- 1) í•´ë‹¹ subject ë¡œë” ë§Œë“¤ê¸° ---
    from torch.utils.data import DataLoader
    from data.code.pkl_dataloader_totact import PKLMultiModalDatasetBaseline
    def _keys_for(split):
        if split == "train": pool = dp.train_keys
        elif split == "val": pool = dp.val_keys
        else: pool = dp.test_keys
        return [k for k in pool if str(k) == str(participant_id)]

    keys = _keys_for(split)
    if not keys:
        print(f"[WARN] subject {participant_id} not found in {split} split")
        return None

    ds_kwargs = {
        'data_map': dp.data_map,
        'veh_cols': dp.veh_cols,
        'fs': cfg.Data.fs,
        'mode': 'act',
        'window_sec': cfg.Data.window_sec_mot,
        'window_stride': cfg.Data.window_stride_mot,
    }
    ds = PKLMultiModalDatasetBaseline(participant_ids=keys, **ds_kwargs)
    loader = DataLoader(ds, batch_size=cfg.Data.batch_size, shuffle=False, collate_fn=collate_fn_baseline)
    fs = int(cfg.Data.fs)

    # --- 2) subjectì˜ ì „ì²´ ì‹œê³„ì—´ ì–»ê¸° (true, pred) ---
    # ë‹¹ì‹ ì´ ì´ë¯¸ ì“°ëŠ” _eval_act_raw_seriesê°€ ìˆë‹¤ë©´ ê·¸ê±¸ ì¨ë„ ì¢‹ê³ ,
    # ì•„ë˜ëŠ” ê·¸ ë¡œì§ì„ inlineìœ¼ë¡œ ì‘ì„±:
    def _pick_preds(out):
        if isinstance(out, dict):
            for k in ('act_preds','preds','act_logits','logits','out'):
                if k in out: return out[k]
            raise ValueError("No ACT outputs")
        return out

    model.eval()
    y_true_all, y_pred_all = [], []
    for batch in tqdm(loader, desc=f"[EVAL] ACT subj={participant_id} ({split})"):
        for k,v in batch.items():
            if torch.is_tensor(v): batch[k] = v.to(cfg.Project.device)
        out = model(batch)
        preds = _pick_preds(out)  # (B,T,1) or (B,T) or (B,T,3)
        if preds.dim()==3 and preds.size(-1)==1: preds = preds.squeeze(-1)
        elif preds.dim()==3: preds = preds[..., 0]
        true = batch.get('label_act', batch.get('label'))
        if true.dim()==3 and true.size(-1)==1: true = true.squeeze(-1)
        if true.dim()==1 and preds.dim()==2 and preds.size(0)==true.size(0):
            true = true.unsqueeze(1)
        if preds.dim()==2 and true.dim()==2 and preds.size(1)!=true.size(1):
            T = min(preds.size(1), true.size(1))
            preds, true = preds[:, :T], true[:, :T]
        mask = (true != -100.0) & torch.isfinite(true) & torch.isfinite(preds)
        if mask.any():
            y_true_all.append(true[mask].detach().cpu())
            y_pred_all.append(preds[mask].detach().cpu())

    if not y_true_all:
        print("[WARN] no valid labels")
        return None

    y_true = torch.cat(y_true_all).numpy()
    y_pred = torch.cat(y_pred_all).numpy()

    # --- 3) smoothing (optional) ---
    y_true_s = _smooth(y_true, fs, smooth_sec)
    y_pred_s = _smooth(y_pred, fs, smooth_sec)

    # --- 4) GLOBAL: ë¡¤ë§ MAE ê¸°ì¤€ top-k best/worst ---
    starts, maes, W = _rolling_mae(y_true_s, y_pred_s, fs, window_sec, step_sec)
    order = np.argsort(maes)
    best_idxs  = order[:top_k]
    worst_idxs = order[::-1][:top_k]

    subj_dir = os.path.join(out_root, f"subj-{participant_id}", split)
    os.makedirs(subj_dir, exist_ok=True)
    summary_rows = []

    for rank, idx in enumerate(best_idxs, 1):
        s, e = int(starts[idx]), int(starts[idx] + W)
        outp = os.path.join(subj_dir, f"global_best_{rank:02d}_{s}-{e}.png")
        _save_case_plot(y_true_s, y_pred_s, fs, s, e, outp,
                        title=f"GLOBAL BEST #{rank} (MAE={maes[idx]:.3f})")
        summary_rows.append(["global_best", rank, s, e, float(maes[idx])])

    for rank, idx in enumerate(worst_idxs, 1):
        s, e = int(starts[idx]), int(starts[idx] + W)
        outp = os.path.join(subj_dir, f"global_worst_{rank:02d}_{s}-{e}.png")
        _save_case_plot(y_true_s, y_pred_s, fs, s, e, outp,
                        title=f"GLOBAL WORST #{rank} (MAE={maes[idx]:.3f})")
        summary_rows.append(["global_worst", rank, s, e, float(maes[idx])])

    # --- 5) SPIKE: ê¸‰ì •ê±° ìŠ¤íŒŒì´í¬ ì¤‘ì‹¬ top-k best/worst ---
    centers = _detect_spikes(y_true_s, fs, thr=spike_thr, min_gap_sec=4.0)
    pad = int(round(fs*spike_pad_sec))
    spike_mae = []
    spike_ranges = []
    for c in centers:
        s, e = max(0, c - pad), min(len(y_true_s), c + pad)
        m = np.abs(y_true_s[s:e] - y_pred_s[s:e]).mean()
        spike_mae.append(m)
        spike_ranges.append((s, e))
    if spike_ranges:
        spike_order = np.argsort(spike_mae)
        s_best  = spike_order[:top_k]
        s_worst = spike_order[::-1][:top_k]
        for rank, k in enumerate(s_best, 1):
            s, e = spike_ranges[k]
            outp = os.path.join(subj_dir, f"spike_best_{rank:02d}_{s}-{e}.png")
            _save_case_plot(y_true_s, y_pred_s, fs, s, e, outp,
                            title=f"SPIKE BEST #{rank} (MAE={spike_mae[k]:.3f})")
            summary_rows.append(["spike_best", rank, int(s), int(e), float(spike_mae[k])])
        for rank, k in enumerate(s_worst, 1):
            s, e = spike_ranges[k]
            outp = os.path.join(subj_dir, f"spike_worst_{rank:02d}_{s}-{e}.png")
            _save_case_plot(y_true_s, y_pred_s, fs, s, e, outp,
                            title=f"SPIKE WORST #{rank} (MAE={spike_mae[k]:.3f})")
            summary_rows.append(["spike_worst", rank, int(s), int(e), float(spike_mae[k])])

    # --- 6) ìš”ì•½ CSV ì €ì¥ ---
    csv_path = os.path.join(subj_dir, "summary.csv")
    with open(csv_path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["bucket","rank","start","end","mae"])
        w.writerows(summary_rows)
    print(f"[SAVE] summary -> {os.path.abspath(csv_path)}")

    return subj_dir

# ============================================================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ (ì‚¬ì „í•™ìŠµë¶€í„° ì‹œì‘)
# ============================================================
def main():
    ensure_dirs()
    cfg = Config()
    set_seed(cfg.Project.seed)
    dp = dataProcessor(cfg); 
    if hasattr(dp, "prepare"): dp.prepare()
    device = cfg.Project.device

    # --------------------------------------------------------
    # STEP 0) ëª¨ì…˜/ê°ì • ì‚¬ì „í•™ìŠµ (ë°˜ë“œì‹œ ì‹¤í–‰)
    # --------------------------------------------------------
    print("\n" + "="*60)
    print("STEP 0) Pretraining Motion / Emotion (start here)")
    print("="*60)

    mot_ckpt = "weights/best_pretrain_motion_imu_veh.pt"
    emo_ckpt = "weights/best_pretrain_emotion_ppg_sc_survey.pt"

    motion_tr = MotionTrainer(cfg, train_keys=dp.train_keys, val_keys=dp.val_keys, test_keys=dp.test_keys)
    emotion_tr = EmotionTrainer(cfg, dp)
    
    # í•™ìŠµ & ì €ì¥
    # try:
    #     motion_tr.train(save_path=mot_ckpt)
    # except TypeError:
    #     motion_tr.train()
    # try:
    #     emotion_tr.train(save_path=emo_ckpt)
    # except TypeError:
    #     emotion_tr.train()

    # ë² ìŠ¤íŠ¸ ë¡œë“œ ë° ê²€ì¦
    if os.path.exists(mot_ckpt):
        state = torch.load(mot_ckpt, map_location="cpu")
        motion_tr.load(state["model_state_dict"] if isinstance(state, dict) and "model_state_dict" in state else state)
    if os.path.exists(emo_ckpt):
        state = torch.load(emo_ckpt, map_location="cpu")
        emotion_tr.load(state["model_state_dict"] if isinstance(state, dict) and "model_state_dict" in state else state)

    if hasattr(motion_tr, "evaluate"):
        try: motion_tr.evaluate(split="val")
        except TypeError: motion_tr.evaluate()
    if hasattr(emotion_tr, "evaluate"):
        try: emotion_tr.evaluate(split="val")
        except TypeError: emotion_tr.evaluate()

    # --------------------------------------------------------
    # STEP 1) ì»¨í…ìŠ¤íŠ¸ ì „ë¬¸ê°€ (v28 í“¨ì „: ê°ì •/ëª¨ì…˜)
    #   - ê°€ëŠ¥í•˜ë©´ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì…
    # --------------------------------------------------------
    print("\n" + "="*60)
    print("STEP 1) Train Context Expert (Fusion v28)")
    print("="*60)

    context_expert = ContextExpertTrainer(cfg, dp.train_keys, dp.val_keys, dp.test_keys)

    # ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ì „ë‹¬: íŠ¸ë ˆì´ë„ˆê°€ í•´ë‹¹ APIë¥¼ ì œê³µí•  ê²½ìš° ìë™ ì£¼ì…
    injected = False
    if hasattr(context_expert, "load_from_pretrained"):
        try:
            context_expert.load_from_pretrained(motion_ckpt=mot_ckpt, emotion_ckpt=emo_ckpt)
            injected = True
            print("[Fusion] loaded pretrain via load_from_pretrained")
        except Exception as e:
            print(f"[Fusion] pretrain injection skipped: {e}")
    elif all(hasattr(context_expert, attr) for attr in ["motion_backbone", "emotion_backbone"]):
        try:
            if os.path.exists(mot_ckpt):
                state = torch.load(mot_ckpt, map_location="cpu")
                msd = state["model_state_dict"] if isinstance(state, dict) and "model_state_dict" in state else state
                context_expert.motion_backbone.load_state_dict(msd, strict=False)
            if os.path.exists(emo_ckpt):
                state = torch.load(emo_ckpt, map_location="cpu")
                esd = state["model_state_dict"] if isinstance(state, dict) and "model_state_dict" in state else state
                context_expert.emotion_backbone.load_state_dict(esd, strict=False)
            injected = True
            print("[Fusion] loaded pretrain into motion_backbone/emotion_backbone")
        except Exception as e:
            print(f"[Fusion] backbone load skipped: {e}")

    fusion_ckpt = "weights/best_fusion_v28.pt"
    #context_expert.fusion_train(save_path=fusion_ckpt)

    if os.path.exists(fusion_ckpt):
        state = torch.load(fusion_ckpt, map_location="cpu")
        context_expert.load_state_dict(state["model_state_dict"] if isinstance(state, dict) and "model_state_dict" in state else state)

    # ê²€ì¦ ì„±ëŠ¥
    va_loader = make_multitask_loader(cfg, dp.val_keys, shuffle=False, dp=dp)
    try:
        va_mot, va_v, va_a = context_expert.evaluate(va_loader)
        print(f"[Fusion v28] VAL -> Motion: {va_mot:.4f} | Valence: {va_v:.4f} | Arousal: {va_a:.4f}")
    except Exception:
        _ = context_expert.evaluate(va_loader)

    # --------------------------------------------------------
    # STEP 2-1) TOT/ACT ë² ì´ìŠ¤ë¼ì¸ í•™ìŠµ
    # --------------------------------------------------------
    print("\n" + "="*60)
    print("STEP 2-1) Train TOT/ACT Baselines")
    print("="*60)

    tot_train_loader, tot_val_loader, tot_test_loader = make_totact_loaders(cfg, dp, "tot", batch_size=cfg.Data.batch_size)
    act_train_loader, act_val_loader, act_test_loader = make_totact_loaders(cfg, dp, "act", batch_size=cfg.Data.batch_size)

    tot_baseline = TOT_BaselineAblation(cfg).to(device)
    print(f"[TOT] using modalities: {cfg.TOT.use_modalities} (feat_dim={cfg.TOT.feat_dim}, attn_pool={cfg.TOT.attn_pool})")
    act_baseline = ACT_BaselineAblation(cfg).to(device)
    
    BaselineTrainer(tot_baseline, cfg, tot_train_loader, tot_val_loader, "tot", num_classes=3).train(test_loader=tot_test_loader)  # saves weights/best_baseline_tot.pt
    #BaselineTrainer(act_baseline, cfg, act_train_loader, act_val_loader, "act", num_classes=3).train(test_loader=act_test_loader)   # saves weights/best_baseline_act.pt

    # --------------------------------------------------------
    # STEP 2-2) ì—”í•¸ì„œ íŒŒì¸íŠœë‹ (ë² ì´ìŠ¤ë¼ì¸ ë™ê²° + í“¨ì „ íŠ¹ì„± ì¶”ê°€ ì…ë ¥)
    # --------------------------------------------------------
    print("\n" + "="*60)
    print("STEP 2-2) Fine-tune with Fused Context (freeze baselines)")
    print("="*60)

    tot_baseline_ckpt = "weights/best_baseline_tot.pt"
    act_baseline_ckpt = "weights/best_baseline_act.pt"
    if os.path.exists(tot_baseline_ckpt):
        tot_baseline.load_state_dict(torch.load(tot_baseline_ckpt, map_location="cpu"))
    if os.path.exists(act_baseline_ckpt):
        act_baseline.load_state_dict(torch.load(act_baseline_ckpt, map_location="cpu"))
    tot_baseline.eval(); act_baseline.eval()

    enhancer_tot = EnhancedTOTModel(cfg, tot_baseline, context_expert).to(device)
    enhancer_act = EnhancedACTModel(cfg, act_baseline, context_expert).to(device)

    enh_tot_ckpt = "weights/best_enhancer_tot.pt"
    enh_act_ckpt = "weights/best_enhancer_act.pt"
    EnhancerTrainer(enhancer_tot, cfg, tot_train_loader, tot_val_loader, "tot", num_classes=3).train(save_path=enh_tot_ckpt)
    EnhancerTrainer(enhancer_act, cfg, act_train_loader, act_val_loader, "act", num_classes=3).train(save_path=enh_act_ckpt)

    if os.path.exists(enh_tot_ckpt):
        enhancer_tot.load_state_dict(torch.load(enh_tot_ckpt, map_location="cpu"))
    if os.path.exists(enh_act_ckpt):
        enhancer_act.load_state_dict(torch.load(enh_act_ckpt, map_location="cpu"))

    # --------------------------------------------------------
    # STEP 3) ìµœì¢… í‰ê°€ + ì‹œê°í™”
    # --------------------------------------------------------
    print("\n" + "="*60)
    print("STEP 3) Final Evaluation & Visualization")
    print("="*60)

    # Baseline or Enhancer ì•„ë¬´ê±°ë‚˜ ëª¨ë¸/ë¡œë” ë„£ì–´ ê°ì‚¬ ê°€ëŠ¥
    stats_val, ytrue_hist_val, ypred_hist_val = audit_tot_split(
        model=enhancer_tot,   # baseline_tot ë„ ê°€ëŠ¥
        loader=tot_val_loader,
        device=cfg.Project.device,
        pooling="mean",
        desc="[AUDIT] TOT Val"
    )

    stats_test, ytrue_hist_test, ypred_hist_test = audit_tot_split(
        model=enhancer_tot,
        loader=tot_test_loader,
        device=cfg.Project.device,
        pooling="mean",
        desc="[AUDIT] TOT Test"
    )


    te_loader = make_multitask_loader(cfg, dp.test_keys, shuffle=False, dp=dp)
    try:
        test_mot, test_v, test_a = context_expert.evaluate(te_loader)
    except Exception:
        out = context_expert.evaluate(te_loader)
        test_mot, test_v, test_a = (out + (float("nan"),) * 3)[:3]

    print(f"[Fusion v28] TEST -> Motion: {test_mot:.4f} | Valence: {test_v:.4f} | Arousal: {test_a:.4f}")

    test_acc_tot, (tot_true, tot_pred) = evaluate_enhancer_tot(enhancer_tot, tot_test_loader, device)

    test_mse_min_act, test_rmse_mean_act, act_true, act_pred = evaluate_enhancer_act(enhancer_act, act_test_loader, device)

    if tot_true is not None and tot_pred is not None:
        save_confusion(tot_true, tot_pred, "results/tot_confusion.png", title="TOT Confusion Matrix")
    if act_true is not None and act_pred is not None:
        save_act_line_plot(act_true, act_pred, "results/act_prediction_plot.png")

    print("\n" + "="*40)
    print("FINAL TEST RESULTS")
    print("="*40)
    print(f"Emotion/Motion -> Motion Acc: {test_mot:.4f}, Valence Acc: {test_v:.4f}, Arousal Acc: {test_a:.4f}")
    print(f"Enhanced TOT   -> TOT Acc: {test_acc_tot:.4f}")
    print(f"Enhanced ACT   -> min ACT MSE: {test_mse_min_act:.4f}")
    print(f"Enhanced ACT   -> mean ACT RMSE: {test_rmse_mean_act:.4f}")
    print("="*40)
    print("ğŸ“Š Plots saved under ./results")

    plot_act_for_subject(enhancer_act, cfg, dp, participant_id="10", split="test", out_dir="results")
    # ì˜ˆ: test split ì „ì²´ ë¡œë”ë¡œ ì‹œë‚˜ë¦¬ì˜¤ë³„ í”Œë¡¯ ì €ì¥
    saved_paths = plot_act_by_scenario(enhancer_act, act_test_loader, device=cfg.Project.device,
                                   out_dir="results", prefix="act_test_scn")
    plot_act_for_subject(enhancer_act, cfg, dp, participant_id="16", split="val", out_dir="results")

    # ì˜ˆ: ê²€ì¦ì…‹, subject '16'ì— ëŒ€í•´ ì¼€ì´ìŠ¤ ìŠ¤í„°ë”” ì €ì¥
    case_dir = make_act_case_studies(
        model=enhancer_act, cfg=cfg, dp=dp, participant_id="16", split="val",
        window_sec=8.0, step_sec=2.0, top_k=5,
        smooth_sec=0.25,          # 0~0.5s ì •ë„ ë¶€ë“œëŸ½ê²Œ (ì˜µì…˜)
        spike_thr=14.0,           # ê¸‰ì •ê±° ACT ìƒí•œ ê·¼ì²˜
        spike_pad_sec=3.0,        # ìŠ¤íŒŒì´í¬ ì£¼ë³€ Â±3ì´ˆ
        out_root="results/case_studies"
    )
    print("saved to:", case_dir)

if __name__ == "__main__":
    main()
